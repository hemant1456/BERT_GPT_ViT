# BERT, GPT, and ViT Implementations

This repository contains implementations of BERT (Bidirectional Encoder Representations from Transformers), GPT (Generative Pre-trained Transformer), and ViT (Vision Transformer). These implementations demonstrate the architecture and functionality of each model, with examples of how to train and apply these models to real-world data.


## File Descriptions

- `BERT.py` - The implementation of the BERT model.
- `ViT.py` - The implementation of the Vision Transformer model.
- `attention_block.py` - Utility file for attention mechanisms used in the models.
- `embedding.py` - Contains embedding layer implementations.
- `dataloader_BERT.py` - Dataloader for processing text data suitable for BERT.
- `dataloader_vit.py` - Dataloader for processing image data suitable for ViT.
- `training.txt` - The dataset on which the BERT model was trained.

### Installation

Clone this repository and install the required packages:

```bash
git clone https://github.com/hemant1456/BERT_GPT_ViT.git
cd BERT_GPT_ViT
pip install -r requirements.txt
